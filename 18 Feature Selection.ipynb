{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "While it is good to try to engineer features that try to capture some latent representations and patterns in the underlying data, it is not always a good thing to deal with feature sets having maybe thousands of features or even more. Dealing with a large number of features bring us to the concept of the curse of dimensionality.\n",
    "\n",
    "More features tend to make models more complex and difficult to interpret. Besides this, it can often lead to models over-fitting on the training data. This basically leads to a very specialized model tuned only to the data which it used for training and hence even if you get a high model performance, it\n",
    "will end up performing very poorly on new, previously unseen data. \n",
    "\n",
    "The ultimate objective is to select an optimal number of features to train and build models that generalize very well on the data and prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets demonstrate feature selection \n",
    "\n",
    "# # Import necessary dependencies and settings\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold-Based Methods"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is a filter based feature selection strategy, where you can use some form of cut-off or thresholding for limiting the total number of features during feature selection. \n",
    "\n",
    "Thresholds can be of various forms. Some of them can be used during the feature engineering process itself, where you can specify threshold parameters.\n",
    "\n",
    "A simple example of this would be to limit feature terms in the Bag of Words model, which we used for text based feature engineering earlier. \n",
    "\n",
    "The scikit-learn framework provides parameters like min_df and max_df which can be used to specify thresholds for ignoring terms which have document frequency above and below user specified thresholds. The following snippet depicts a way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=0.85, max_features=2000, min_df=0.1,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "# ## Limiting features in bag of word based models\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(min_df=0.1, max_df=0.85, max_features=2000)\n",
    "# 2000 is our decision to give.it can be increased if reqd\n",
    "# discard if word count is less than 10% and greater than 85%\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This basically builds a count vectorizer which ignores feature terms which occur in less than 10% of the total corpus and also ignores terms which occur in more than 85% of the total corpus. Besides this we also put a hard limit of 2000 maximum features in the feature set.\n",
    "\n",
    "variance based thresholding\n",
    "---------------------------\n",
    "Another way of using thresholds is to use variance based thresholding where features having low variance (below a user-specified threshold) are removed. This signifies that we want to remove features that have values that are more or less constant across all the observations in our datasets. We can apply this to our Pok√©mon dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gen 1  Gen 2  Gen 3  Gen 4  Gen 5  Gen 6\n",
      "0      1      0      0      0      0      0\n",
      "1      1      0      0      0      0      0\n",
      "2      1      0      0      0      0      0\n",
      "3      1      0      0      0      0      0\n",
      "4      1      0      0      0      0      0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets_n_images/datasets_module_4/Pokemon.csv')\n",
    "poke_gen = pd.get_dummies(df['Generation'])\n",
    "print(poke_gen.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VarianceThreshold(threshold=0.15)\n"
     ]
    }
   ],
   "source": [
    "# Next, we want to remove features from the one hot encoded features \n",
    "# where the variance is less than 0.15. \n",
    "# We can do this using the following snippet.\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "vt = VarianceThreshold(threshold=.15)\n",
    "print(vt.fit(poke_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gen 1</th>\n",
       "      <th>Gen 2</th>\n",
       "      <th>Gen 3</th>\n",
       "      <th>Gen 4</th>\n",
       "      <th>Gen 5</th>\n",
       "      <th>Gen 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>variance</td>\n",
       "      <td>0.164444</td>\n",
       "      <td>0.114944</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.128373</td>\n",
       "      <td>0.163711</td>\n",
       "      <td>0.0919937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>select_feature</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Gen 1     Gen 2 Gen 3     Gen 4     Gen 5      Gen 6\n",
       "variance        0.164444  0.114944  0.16  0.128373  0.163711  0.0919937\n",
       "select_feature      True     False  True     False      True      False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To view the variances as well as which features were finally \n",
    "# selected by this algorithm, we can use the variances_ property and the \n",
    "# get_support(...) function respectively. \n",
    "# The following snippet depicts this clearly in a formatted dataframe.\n",
    "\n",
    "pd.DataFrame({'variance': vt.variances_,\n",
    "'select_feature': vt.get_support()},\n",
    " index=poke_gen.columns).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see which features have been selected based on their True values and also their variance being above 0.15. To get the final subset of selected features, you can use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gen 1  Gen 3  Gen 5\n",
      "0      1      0      0\n",
      "1      1      0      0\n",
      "2      1      0      0\n",
      "3      1      0      0\n",
      "4      1      0      0\n"
     ]
    }
   ],
   "source": [
    "poke_gen_subset = poke_gen.iloc[:,vt.get_support()].head()\n",
    "print(poke_gen_subset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
